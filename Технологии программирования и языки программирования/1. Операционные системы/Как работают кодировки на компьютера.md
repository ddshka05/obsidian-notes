Компьютер - это машина, которая работает с числами. Человек же работает с текстом. Чтобы подружить текст с компьютером придумали коды для символов. 

Самое главное, чтобы передать сообщение, нужно иметь одинаковые таблицы кодов символов, чтобы сообщение между компьютера было возможным. Поэтому существуют разработанные общие стандарты кодировок, которые использует каждый компьютер.
### ASCII
Развитие компьютеров бурно проходило в Америке, поэтому они создали для английского алфавита кодировку ASCII (American standart code for information interchange). Всего 1 символ в кодировке ASCII требует 7 бит, что позволяет закодировать 128 значений. Первый бит в байте соответственно свободный и равен нулю. Первые 31 символ - управляющие (\n - например). 
### Нехватка кодов
Из-за разрастания популярности компьютеров по всему миру появилась потребность в новых кодах, которые бы кодировали местные языки.
### Кодовые страницы
Первым решением стало создание кодовых страниц. 1Б умещает в себя 256 символов, что позволяет закодировать некоторые языки. Так первые 128 символов совпадали с ASCII, а остальные были заполнены символами конкретного языка. 

Некоторые страны создавали несколько кодовых страниц для своих языков, что выливалось в полный бардак. Да и все возможные языки нельзя закодировать 128 символами - например Китайский с его иероглификой. Проблема так же усугублялась развитием интернета и глобальным обменом файлов. 
### Windows с ANSI
Винда, выйдя на первый план, сделала свои кодовые страницы для ОС - ANSI (american national standarts institute), где в названии windows-125X, X обозначал номер страны (для кириллицы - 1). Дойдя до Азии поняли, что кодовыми страницами проблему не решить. Плюс проблема стандартизации так же стояла остро.
### Unicode
Это стандарт, которые был создан для решения проблем со стандартизацией. Теперь нет единого кол-ва кодирования байт (от 2 байт), что позволило закодировать оч много языков, смайликов и т.п., но там появились другие проблемы. 
### Проблемы, выявленные при внедрении Unicode
Например, в англоязычной среде считали Unicode слишком расточительной, зачем тратить на кодировку 16 бит, если можно за 7 бит все закодировать, поэтому её игнорировали. Но одной из самых насущных проблем стала проблема порядка хранения байт.

Т.к. есть два типа порядка хранения данных ([[Как работает память компьютера]]), то появилась потребность в создании кодировок, которые бы правильно расшифровывали текст с разных компьютеров, реализующих разный подход к хранению данных в оперативной памяти. 

![[Pasted image 20250704103715.png]]

### UCS-2
Она добавляла два байта (BOM-байты, где BOM - byte order mark) вперед, для определения порядка хранения. Но это было снова расточительно по памяти!!! 
### UTF-8
Такой универсальной и классной кодировкой стала UTF-8, где коды символов не фиксированы. 8 в названии - это минимальное длинна в битах, первый 128 кодов совпадают с ASCII, далее занимают от 2 до 4 байтами.

Кодируют так:
Существует 4 макси для кодов символов (0, 110, 1110, 11110) - эти последовательности битов обозначают, сколько надо прочитать байт для определения символов.

![[Pasted image 20250704104448.png]]

Таким боком можно не использовать бум байты. Вот пример:

![[Pasted image 20250704104735.png]]

То есть из юникода перевели в UTF-8 таблицу, понимая откуда начинается символ, получатся можно без проблем расшифровывать с разных компьютеров с разной памятью. 

Но этот динамический размер символов привел к увеличению времени обработки текста, поэтому снова не так хорошо переходили на эту кодировку. 